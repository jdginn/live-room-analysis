{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Success Case Analysis\n",
    "\n",
    "This notebook automates deep analysis of when and why some experiment configurations produce BOTH:\n",
    "- **Low DrumDeadT30** (e.g., < 200ms) - indicating good sound absorption in the dead zone\n",
    "- **High DrumLiveT30** (e.g., > 250ms) - indicating good reverberation in the live zone\n",
    "\n",
    "## Features\n",
    "- Load cleaned experiment DataFrame from `experiment_metrics.csv`\n",
    "- Select/adjust thresholds for DrumDeadT30 and DrumLiveT30 interactively\n",
    "- Filter data for 'success cases' meeting BOTH thresholds\n",
    "- Compare input/derived parameters between success and other configs\n",
    "- Visualize key parameter distributions (hist, KDE, boxplot, violin)\n",
    "- Statistical tests for differences\n",
    "- Train interpretable classifier (DecisionTree, RandomForest, Logistic Regression)\n",
    "- Feature importances and SHAP analysis\n",
    "- Rule extraction: print decision tree rules\n",
    "- Scatterplots/parallel coordinates to visualize success regions\n",
    "- Output candidate parameter sets for new experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "import warnings\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# SHAP for model interpretability\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"SHAP not available. Install with: pip install shap\")\n",
    "\n",
    "# Interactive widgets (optional)\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WIDGETS_AVAILABLE = False\n",
    "    print(\"ipywidgets not available. Install with: pip install ipywidgets\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the experiment metrics data\n",
    "df = pd.read_csv('experiment_metrics.csv')\n",
    "\n",
    "print(f\"Loaded {len(df)} experiments\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nData shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns\n",
    "INPUT_FEATURES = [\n",
    "    'l_num_reflectors', 'l_reflector_angle', 'l_reflector_depth',\n",
    "    'l_start_offset', 'l_finish_offset',\n",
    "    'r_num_reflectors', 'r_reflector_angle', 'r_reflector_depth',\n",
    "    'r_start_offset', 'r_finish_offset'\n",
    "]\n",
    "\n",
    "MAPPED_FEATURES = [\n",
    "        \"LA_angle\",\n",
    "        \"SA_angle\",\n",
    "        \"LA_num_reflectors\",\n",
    "        \"SA_num_reflectors\",\n",
    "        \"LA_depth\",\n",
    "        \"SA_depth\",\n",
    "        \"LA_start_offset\",\n",
    "        \"SA_start_offset\",\n",
    "        \"LA_finish_offset\",\n",
    "        \"SA_finish_offset\",\n",
    "]\n",
    "\n",
    "DERIVED_FEATURES = [\n",
    "        #\"avg_angle\",\n",
    "        \"total_depth\",\n",
    "        \"avg_depth\",\n",
    "        \"total_reflectors\",\n",
    "        \"angle_diff\",\n",
    "        \"depth_diff\",\n",
    "        \"num_reflectors_diff\",\n",
    "        \"start_offset_diff\",\n",
    "        \"finish_offset_diff\",\n",
    "]\n",
    "\n",
    "\n",
    "ALL_FEATURES = MAPPED_FEATURES + DERIVED_FEATURES\n",
    "\n",
    "# Verify which features exist in the dataframe\n",
    "available_features = [f for f in ALL_FEATURES if f in df.columns]\n",
    "print(f\"Available features: {len(available_features)} of {len(ALL_FEATURES)}\")\n",
    "print(f\"Missing features: {set(ALL_FEATURES) - set(available_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for target metrics\n",
    "print(\"Target Metrics Summary:\")\n",
    "print(\"=\"*60)\n",
    "for metric in ['DrumDeadT30', 'DrumLiveT30']:\n",
    "    if metric in df.columns:\n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(f\"  Min: {df[metric].min():.2f} ms\")\n",
    "        print(f\"  Max: {df[metric].max():.2f} ms\")\n",
    "        print(f\"  Mean: {df[metric].mean():.2f} ms\")\n",
    "        print(f\"  Median: {df[metric].median():.2f} ms\")\n",
    "        print(f\"  Std: {df[metric].std():.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Success Thresholds (Interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default thresholds based on problem statement\n",
    "DEFAULT_DEAD_T30_THRESHOLD = 140  # ms (max for \"success\")\n",
    "DEFAULT_LIVE_T30_THRESHOLD = 250  # ms (min for \"success\")\n",
    "\n",
    "# Initialize threshold values\n",
    "dead_t30_threshold = DEFAULT_DEAD_T30_THRESHOLD\n",
    "live_t30_threshold = DEFAULT_LIVE_T30_THRESHOLD\n",
    "\n",
    "def create_success_mask(df: pd.DataFrame, dead_thresh: float, live_thresh: float) -> pd.Series:\n",
    "    \"\"\"Create a boolean mask for success cases.\"\"\"\n",
    "    return (df['DrumDeadT30'] < dead_thresh) & (df['DrumLiveT30'] > live_thresh)\n",
    "\n",
    "def count_successes(df: pd.DataFrame, dead_thresh: float, live_thresh: float) -> Tuple[int, int, float]:\n",
    "    \"\"\"Count success cases and return statistics.\"\"\"\n",
    "    mask = create_success_mask(df, dead_thresh, live_thresh)\n",
    "    n_success = mask.sum()\n",
    "    n_total = len(df)\n",
    "    pct = 100 * n_success / n_total\n",
    "    return n_success, n_total, pct\n",
    "\n",
    "# Show initial counts\n",
    "n_success, n_total, pct = count_successes(df, dead_t30_threshold, live_t30_threshold)\n",
    "print(f\"With thresholds: DrumDeadT30 < {dead_t30_threshold}ms, DrumLiveT30 > {live_t30_threshold}ms\")\n",
    "print(f\"Success cases: {n_success} / {n_total} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive threshold adjustment (if widgets available)\n",
    "if WIDGETS_AVAILABLE:\n",
    "    dead_slider = widgets.FloatSlider(\n",
    "        value=DEFAULT_DEAD_T30_THRESHOLD,\n",
    "        min=df['DrumDeadT30'].min(),\n",
    "        max=df['DrumDeadT30'].max(),\n",
    "        step=5,\n",
    "        description='DrumDeadT30 <',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    live_slider = widgets.FloatSlider(\n",
    "        value=DEFAULT_LIVE_T30_THRESHOLD,\n",
    "        min=df['DrumLiveT30'].min(),\n",
    "        max=df['DrumLiveT30'].max(),\n",
    "        step=5,\n",
    "        description='DrumLiveT30 >',\n",
    "        continuous_update=False\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def update_thresholds(change):\n",
    "        global dead_t30_threshold, live_t30_threshold\n",
    "        dead_t30_threshold = dead_slider.value\n",
    "        live_t30_threshold = live_slider.value\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            n_success, n_total, pct = count_successes(df, dead_t30_threshold, live_t30_threshold)\n",
    "            print(f\"Success cases: {n_success} / {n_total} ({pct:.1f}%)\")\n",
    "    \n",
    "    dead_slider.observe(update_thresholds, names='value')\n",
    "    live_slider.observe(update_thresholds, names='value')\n",
    "    \n",
    "    display(widgets.VBox([dead_slider, live_slider, output]))\n",
    "    update_thresholds(None)\n",
    "else:\n",
    "    print(\"Interactive widgets not available. Modify thresholds manually below:\")\n",
    "    print(f\"dead_t30_threshold = {dead_t30_threshold}\")\n",
    "    print(f\"live_t30_threshold = {live_t30_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual threshold adjustment (modify these values as needed)\n",
    "# Uncomment and modify to override interactive sliders\n",
    "# dead_t30_threshold = 200\n",
    "# live_t30_threshold = 250\n",
    "\n",
    "print(f\"\\nFinal thresholds: DrumDeadT30 < {dead_t30_threshold}ms, DrumLiveT30 > {live_t30_threshold}ms\")\n",
    "n_success, n_total, pct = count_successes(df, dead_t30_threshold, live_t30_threshold)\n",
    "print(f\"Success cases: {n_success} / {n_total} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Success Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create success label\n",
    "df['is_success'] = create_success_mask(df, dead_t30_threshold, live_t30_threshold)\n",
    "\n",
    "# Split data\n",
    "df_success = df[df['is_success']].copy()\n",
    "df_other = df[~df['is_success']].copy()\n",
    "\n",
    "print(f\"Success cases: {len(df_success)}\")\n",
    "print(f\"Other cases: {len(df_other)}\")\n",
    "\n",
    "# Display success cases\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUCCESS CASES:\")\n",
    "print(\"=\"*60)\n",
    "success_display = df_success[['experiment', 'DrumDeadT30', 'DrumLiveT30'] + available_features[:5]].head(20)\n",
    "display(success_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Parameters: Success vs Other Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_groups(df_success: pd.DataFrame, df_other: pd.DataFrame, \n",
    "                   features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Compare feature statistics between success and other groups.\"\"\"\n",
    "    comparison = []\n",
    "    \n",
    "    for feature in features:\n",
    "        if feature not in df_success.columns:\n",
    "            continue\n",
    "            \n",
    "        success_vals = df_success[feature].dropna()\n",
    "        other_vals = df_other[feature].dropna()\n",
    "        \n",
    "        if len(success_vals) < 2 or len(other_vals) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Statistical test (Mann-Whitney U for non-parametric comparison)\n",
    "        stat, p_value = stats.mannwhitneyu(success_vals, other_vals, alternative='two-sided')\n",
    "        \n",
    "        # Effect size (Cohen's d approximation)\n",
    "        pooled_std = np.sqrt((success_vals.std()**2 + other_vals.std()**2) / 2)\n",
    "        if pooled_std > 0:\n",
    "            cohens_d = (success_vals.mean() - other_vals.mean()) / pooled_std\n",
    "        else:\n",
    "            cohens_d = 0\n",
    "        \n",
    "        comparison.append({\n",
    "            'feature': feature,\n",
    "            'success_mean': success_vals.mean(),\n",
    "            'success_std': success_vals.std(),\n",
    "            'other_mean': other_vals.mean(),\n",
    "            'other_std': other_vals.std(),\n",
    "            'diff_mean': success_vals.mean() - other_vals.mean(),\n",
    "            'cohens_d': cohens_d,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(comparison).sort_values('p_value')\n",
    "\n",
    "comparison_df = compare_groups(df_success, df_other, available_features)\n",
    "print(\"Parameter Comparison: Success vs Other Configurations\")\n",
    "print(\"=\"*80)\n",
    "display(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight significant differences\n",
    "significant_features = comparison_df[comparison_df['significant']]['feature'].tolist()\n",
    "print(f\"\\nFeatures with significant differences (p < 0.05): {len(significant_features)}\")\n",
    "for feat in significant_features:\n",
    "    row = comparison_df[comparison_df['feature'] == feat].iloc[0]\n",
    "    direction = \"higher\" if row['diff_mean'] > 0 else \"lower\"\n",
    "    print(f\"  - {feat}: Success cases are {direction} (diff={row['diff_mean']:.3f}, p={row['p_value']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Parameter Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_comparison(df: pd.DataFrame, feature: str, \n",
    "                                  figsize: tuple = (14, 4)) -> plt.Figure:\n",
    "    \"\"\"Plot histograms, KDE, boxplot, and violin plots for a feature.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=figsize)\n",
    "    \n",
    "    success_vals = df[df['is_success']][feature].dropna()\n",
    "    other_vals = df[~df['is_success']][feature].dropna()\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(other_vals, bins=20, alpha=0.6, label='Other', color='gray')\n",
    "    axes[0].hist(success_vals, bins=20, alpha=0.8, label='Success', color='green')\n",
    "    axes[0].set_xlabel(feature)\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Histogram')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # KDE\n",
    "    if len(success_vals) > 1 and len(other_vals) > 1:\n",
    "        sns.kdeplot(other_vals, ax=axes[1], label='Other', color='gray', fill=True, alpha=0.3)\n",
    "        sns.kdeplot(success_vals, ax=axes[1], label='Success', color='green', fill=True, alpha=0.5)\n",
    "    axes[1].set_xlabel(feature)\n",
    "    axes[1].set_title('KDE')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    # Boxplot\n",
    "    box_data = [other_vals, success_vals]\n",
    "    bp = axes[2].boxplot(box_data, labels=['Other', 'Success'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor('gray')\n",
    "    bp['boxes'][1].set_facecolor('green')\n",
    "    axes[2].set_ylabel(feature)\n",
    "    axes[2].set_title('Boxplot')\n",
    "    \n",
    "    # Violin plot\n",
    "    sns.violinplot(x='is_success', y=feature, data=df, ax=axes[3], \n",
    "                   hue='is_success', palette={True: 'green', False: 'gray'}, legend=False)\n",
    "    axes[3].set_xticklabels(['Other', 'Success'])\n",
    "    axes[3].set_xlabel('')\n",
    "    axes[3].set_title('Violin Plot')\n",
    "    \n",
    "    plt.suptitle(f'Distribution Comparison: {feature}', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot distributions for top significant features\n",
    "top_features = comparison_df.head(6)['feature'].tolist()\n",
    "for feature in top_features:\n",
    "    plot_distribution_comparison(df, feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary violin plot for all features\n",
    "n_features = len(available_features)\n",
    "n_cols = 4\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 4*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(available_features):\n",
    "    if feature in df.columns:\n",
    "        sns.violinplot(x='is_success', y=feature, data=df, ax=axes[idx],\n",
    "                      hue='is_success', palette={True: 'green', False: 'gray'}, legend=False)\n",
    "        axes[idx].set_xticklabels(['Other', 'Success'])\n",
    "        axes[idx].set_xlabel('')\n",
    "        axes[idx].set_title(feature, fontsize=10)\n",
    "\n",
    "# Hide unused axes\n",
    "for idx in range(len(available_features), len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.suptitle('All Features: Success vs Other Distributions', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Statistical Tests for Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_statistical_tests(df_success: pd.DataFrame, df_other: pd.DataFrame,\n",
    "                          features: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Run multiple statistical tests comparing groups.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for feature in features:\n",
    "        if feature not in df_success.columns:\n",
    "            continue\n",
    "            \n",
    "        success_vals = df_success[feature].dropna()\n",
    "        other_vals = df_other[feature].dropna()\n",
    "        \n",
    "        if len(success_vals) < 3 or len(other_vals) < 3:\n",
    "            continue\n",
    "        \n",
    "        # Mann-Whitney U test (non-parametric)\n",
    "        mw_stat, mw_p = stats.mannwhitneyu(success_vals, other_vals, alternative='two-sided')\n",
    "        \n",
    "        # T-test (parametric)\n",
    "        t_stat, t_p = stats.ttest_ind(success_vals, other_vals)\n",
    "        \n",
    "        # Kolmogorov-Smirnov test (distribution difference)\n",
    "        ks_stat, ks_p = stats.ks_2samp(success_vals, other_vals)\n",
    "        \n",
    "        results.append({\n",
    "            'feature': feature,\n",
    "            'mw_stat': mw_stat,\n",
    "            'mw_p_value': mw_p,\n",
    "            't_stat': t_stat,\n",
    "            't_p_value': t_p,\n",
    "            'ks_stat': ks_stat,\n",
    "            'ks_p_value': ks_p,\n",
    "            'any_significant': (mw_p < 0.05) or (t_p < 0.05) or (ks_p < 0.05)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values('mw_p_value')\n",
    "\n",
    "stat_tests = run_statistical_tests(df_success, df_other, available_features)\n",
    "print(\"Statistical Tests: Success vs Other\")\n",
    "print(\"=\"*100)\n",
    "print(\"MW = Mann-Whitney U, T = T-test, KS = Kolmogorov-Smirnov\")\n",
    "print()\n",
    "display(stat_tests.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Interpretable Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classification\n",
    "X = df[available_features].copy()\n",
    "y = df['is_success'].astype(int)\n",
    "\n",
    "# Handle missing values\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples ({y_train.sum()} success)\")\n",
    "print(f\"Test set: {len(X_test)} samples ({y_test.sum()} success)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features for Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train classifiers\n",
    "classifiers = {\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    if name == 'Logistic Regression':\n",
    "        clf.fit(X_train_scaled, y_train)\n",
    "        y_pred = clf.predict(X_test_scaled)\n",
    "        cv_scores = cross_val_score(clf, X_train_scaled, y_train, cv=5)\n",
    "    else:\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        cv_scores = cross_val_score(clf, X_train, y_train, cv=5)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': clf,\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Test Accuracy: {results[name]['accuracy']:.4f}\")\n",
    "    print(f\"  CV Accuracy: {results[name]['cv_mean']:.4f} (+/- {results[name]['cv_std']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification reports\n",
    "for name, result in results.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name} - Classification Report\")\n",
    "    print(\"=\"*60)\n",
    "    print(classification_report(y_test, result['y_pred'], target_names=['Other', 'Success']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(importance_dict: Dict[str, float], title: str,\n",
    "                              figsize: tuple = (10, 6)) -> plt.Figure:\n",
    "    \"\"\"Plot feature importances as horizontal bar chart.\"\"\"\n",
    "    sorted_items = sorted(importance_dict.items(), key=lambda x: abs(x[1]), reverse=True)\n",
    "    features = [item[0] for item in sorted_items]\n",
    "    importances = [item[1] for item in sorted_items]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    colors = ['green' if imp > 0 else 'red' for imp in importances]\n",
    "    \n",
    "    y_pos = range(len(features))\n",
    "    ax.barh(y_pos, importances, color=colors, alpha=0.7)\n",
    "    ax.set_yticks(y_pos)\n",
    "    ax.set_yticklabels(features)\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title(title)\n",
    "    ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Decision Tree feature importances\n",
    "dt_clf = results['Decision Tree']['model']\n",
    "dt_importances = dict(zip(available_features, dt_clf.feature_importances_))\n",
    "plot_feature_importances(dt_importances, 'Decision Tree Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "# Random Forest feature importances\n",
    "rf_clf = results['Random Forest']['model']\n",
    "rf_importances = dict(zip(available_features, rf_clf.feature_importances_))\n",
    "plot_feature_importances(rf_importances, 'Random Forest Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "# Logistic Regression coefficients (scaled)\n",
    "lr_clf = results['Logistic Regression']['model']\n",
    "lr_importances = dict(zip(available_features, lr_clf.coef_[0]))\n",
    "plot_feature_importances(lr_importances, 'Logistic Regression Coefficients (standardized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined feature importance ranking\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': available_features,\n",
    "    'dt_importance': [dt_importances[f] for f in available_features],\n",
    "    'rf_importance': [rf_importances[f] for f in available_features],\n",
    "    'lr_coef': [abs(lr_importances[f]) for f in available_features]\n",
    "})\n",
    "\n",
    "# Normalize and compute average rank\n",
    "for col in ['dt_importance', 'rf_importance', 'lr_coef']:\n",
    "    importance_df[f'{col}_rank'] = importance_df[col].rank(ascending=False)\n",
    "\n",
    "importance_df['avg_rank'] = importance_df[['dt_importance_rank', 'rf_importance_rank', 'lr_coef_rank']].mean(axis=1)\n",
    "importance_df = importance_df.sort_values('avg_rank')\n",
    "\n",
    "print(\"\\nCombined Feature Importance Ranking:\")\n",
    "print(\"=\"*80)\n",
    "display(importance_df[['feature', 'dt_importance', 'rf_importance', 'lr_coef', 'avg_rank']].round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # SHAP for Random Forest\n",
    "    print(\"Computing SHAP values for Random Forest...\")\n",
    "    explainer_rf = shap.TreeExplainer(rf_clf)\n",
    "    shap_values_rf = explainer_rf.shap_values(X_test)\n",
    "    \n",
    "    # Handle different SHAP versions - newer versions return different format\n",
    "    # For binary classification, get SHAP values for positive class (success)\n",
    "    if isinstance(shap_values_rf, list):\n",
    "        # Older SHAP: returns list of arrays [class_0_values, class_1_values]\n",
    "        shap_values_success = shap_values_rf[1]\n",
    "    else:\n",
    "        # Newer SHAP: returns single array or Explanation object\n",
    "        shap_values_success = shap_values_rf\n",
    "    \n",
    "    # SHAP summary plot for success class\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_success, X_test, feature_names=available_features, show=False)\n",
    "    plt.title('SHAP Summary Plot (Random Forest) - Success Class')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # SHAP bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values_success, X_test, feature_names=available_features, \n",
    "                      plot_type='bar', show=False)\n",
    "    plt.title('SHAP Feature Importance (Random Forest) - Success Class')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"SHAP analysis skipped (shap package not available)\")\n",
    "    print(\"Install with: pip install shap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SHAP_AVAILABLE:\n",
    "    # SHAP dependence plots for top features\n",
    "    top_rf_features = sorted(rf_importances.items(), key=lambda x: x[1], reverse=True)[:4]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Convert shap_values to numpy array if it's an Explanation object\n",
    "    if hasattr(shap_values_success, 'values'):\n",
    "        shap_values_array = shap_values_success.values\n",
    "    else:\n",
    "        shap_values_array = np.array(shap_values_success)\n",
    "    \n",
    "    # Convert X_test to numpy array if it's a DataFrame\n",
    "    X_test_array = X_test.values if hasattr(X_test, 'values') else np.array(X_test)\n",
    "    \n",
    "    for idx, (feature, _) in enumerate(top_rf_features):\n",
    "        feature_idx = available_features.index(feature)\n",
    "        # Use interaction_index=None to avoid the automatic interaction detection that causes errors\n",
    "        shap.dependence_plot(feature_idx, shap_values_array, X_test_array, \n",
    "                            feature_names=available_features, ax=axes[idx], show=False,\n",
    "                            interaction_index=None)\n",
    "        axes[idx].set_title(f'SHAP Dependence: {feature}')\n",
    "    \n",
    "    plt.suptitle('SHAP Dependence Plots for Top Features', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Rule Extraction from Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Decision Tree\n",
    "plt.figure(figsize=(20, 12))\n",
    "plot_tree(dt_clf, feature_names=available_features, class_names=['Other', 'Success'],\n",
    "          filled=True, rounded=True, fontsize=8)\n",
    "plt.title('Decision Tree for Success Classification', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print text rules\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DECISION TREE RULES FOR SUCCESS\")\n",
    "print(\"=\"*80)\n",
    "tree_rules = export_text(dt_clf, feature_names=available_features)\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_success_paths(tree, feature_names: List[str]) -> List[str]:\n",
    "    \"\"\"Extract decision paths that lead to 'Success' classification.\"\"\"\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != -2 else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    \n",
    "    paths = []\n",
    "    \n",
    "    def recurse(node, path):\n",
    "        if tree_.feature[node] != -2:  # Not a leaf\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            \n",
    "            # Left branch (<=)\n",
    "            recurse(tree_.children_left[node], path + [f\"{name} <= {threshold:.2f}\"])\n",
    "            # Right branch (>)\n",
    "            recurse(tree_.children_right[node], path + [f\"{name} > {threshold:.2f}\"])\n",
    "        else:  # Leaf node\n",
    "            # Check if this is a \"success\" leaf (class 1 has more samples)\n",
    "            values = tree_.value[node][0]\n",
    "            if values[1] > values[0]:  # More successes than others\n",
    "                confidence = values[1] / sum(values)\n",
    "                paths.append({\n",
    "                    'rules': path,\n",
    "                    'confidence': confidence,\n",
    "                    'n_success': int(values[1]),\n",
    "                    'n_other': int(values[0])\n",
    "                })\n",
    "    \n",
    "    recurse(0, [])\n",
    "    return paths\n",
    "\n",
    "success_paths = extract_success_paths(dt_clf, available_features)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RULES LEADING TO SUCCESS (High confidence paths)\")\n",
    "print(\"=\"*80)\n",
    "for i, path in enumerate(sorted(success_paths, key=lambda x: x['confidence'], reverse=True)):\n",
    "    print(f\"\\nPath {i+1} (Confidence: {path['confidence']:.1%}, Samples: {path['n_success']} success, {path['n_other']} other):\")\n",
    "    for rule in path['rules']:\n",
    "        print(f\"  - {rule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Success Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of DrumDeadT30 vs DrumLiveT30 with success regions\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "plt.scatter(df[~df['is_success']]['DrumDeadT30'], \n",
    "            df[~df['is_success']]['DrumLiveT30'],\n",
    "            c='gray', alpha=0.5, label='Other', s=50)\n",
    "plt.scatter(df[df['is_success']]['DrumDeadT30'], \n",
    "            df[df['is_success']]['DrumLiveT30'],\n",
    "            c='green', alpha=0.8, label='Success', s=80, edgecolors='black')\n",
    "\n",
    "# Draw threshold lines\n",
    "plt.axvline(x=dead_t30_threshold, color='red', linestyle='--', \n",
    "            label=f'DrumDeadT30 threshold ({dead_t30_threshold}ms)')\n",
    "plt.axhline(y=live_t30_threshold, color='blue', linestyle='--',\n",
    "            label=f'DrumLiveT30 threshold ({live_t30_threshold}ms)')\n",
    "\n",
    "# Shade success region\n",
    "plt.fill_between([df['DrumDeadT30'].min(), dead_t30_threshold],\n",
    "                 live_t30_threshold, df['DrumLiveT30'].max(),\n",
    "                 alpha=0.1, color='green', label='Success Region')\n",
    "\n",
    "plt.xlabel('DrumDeadT30 (ms)')\n",
    "plt.ylabel('DrumLiveT30 (ms)')\n",
    "plt.title('Success Region: Low DrumDeadT30 AND High DrumLiveT30')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for top important features vs targets\n",
    "top_features_for_scatter = importance_df.head(4)['feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(top_features_for_scatter):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create scatter with color by success\n",
    "    scatter = ax.scatter(df[feature], df['DrumDeadT30'], \n",
    "                        c=df['is_success'].map({True: 'green', False: 'gray'}),\n",
    "                        alpha=0.6, s=50)\n",
    "    \n",
    "    ax.axhline(y=dead_t30_threshold, color='red', linestyle='--', alpha=0.7)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('DrumDeadT30 (ms)')\n",
    "    ax.set_title(f'{feature} vs DrumDeadT30')\n",
    "\n",
    "plt.suptitle('Top Features vs DrumDeadT30 (Green = Success)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel coordinates plot\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Select subset of features for readability\n",
    "parallel_features = importance_df.head(6)['feature'].tolist()\n",
    "\n",
    "# Prepare data for parallel coordinates\n",
    "parallel_df = df[parallel_features + ['is_success']].copy()\n",
    "\n",
    "# Normalize features for better visualization\n",
    "for col in parallel_features:\n",
    "    parallel_df[col] = (parallel_df[col] - parallel_df[col].min()) / (parallel_df[col].max() - parallel_df[col].min())\n",
    "\n",
    "parallel_df['Group'] = parallel_df['is_success'].map({True: 'Success', False: 'Other'})\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "parallel_coordinates(parallel_df.drop('is_success', axis=1), 'Group', \n",
    "                     color=['gray', 'green'], alpha=0.5)\n",
    "plt.title('Parallel Coordinates: Success vs Other Configurations')\n",
    "plt.ylabel('Normalized Value')\n",
    "plt.legend(loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairplot for top features\n",
    "pairplot_features = importance_df.head(4)['feature'].tolist()\n",
    "pairplot_df = df[pairplot_features + ['is_success']].copy()\n",
    "\n",
    "g = sns.pairplot(pairplot_df, hue='is_success', \n",
    "                 palette={True: 'green', False: 'gray'},\n",
    "                 diag_kind='kde', plot_kws={'alpha': 0.5})\n",
    "g.fig.suptitle('Pairplot: Top Features by Success', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Output Candidate Parameter Sets for New Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate_parameters(df_success: pd.DataFrame, \n",
    "                                   features: List[str],\n",
    "                                   n_candidates: int = 5,\n",
    "                                   strategy: str = 'mean') -> pd.DataFrame:\n",
    "    \"\"\"Generate candidate parameter sets based on success cases.\n",
    "    \n",
    "    Args:\n",
    "        df_success: DataFrame of success cases\n",
    "        features: List of feature columns\n",
    "        n_candidates: Number of candidate sets to generate\n",
    "        strategy: 'mean', 'median', 'random', or 'optimized'\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with candidate parameter sets\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    if strategy == 'mean':\n",
    "        # Use mean values from success cases\n",
    "        candidate = {f: df_success[f].mean() for f in features if f in df_success.columns}\n",
    "        candidate['strategy'] = 'success_mean'\n",
    "        candidates.append(candidate)\n",
    "        \n",
    "    elif strategy == 'median':\n",
    "        # Use median values from success cases\n",
    "        candidate = {f: df_success[f].median() for f in features if f in df_success.columns}\n",
    "        candidate['strategy'] = 'success_median'\n",
    "        candidates.append(candidate)\n",
    "        \n",
    "    elif strategy == 'random':\n",
    "        # Random samples from success cases\n",
    "        for i in range(min(n_candidates, len(df_success))):\n",
    "            sample = df_success.sample(1).iloc[0]\n",
    "            candidate = {f: sample[f] for f in features if f in df_success.columns}\n",
    "            candidate['strategy'] = f'random_sample_{i+1}'\n",
    "            candidates.append(candidate)\n",
    "            \n",
    "    elif strategy == 'optimized':\n",
    "        # Generate candidates around the best success cases\n",
    "        # Sort by combined objective (low dead, high live)\n",
    "        df_sorted = df_success.copy()\n",
    "        df_sorted['score'] = -df_sorted['DrumDeadT30'] + df_sorted['DrumLiveT30']\n",
    "        df_sorted = df_sorted.sort_values('score', ascending=False)\n",
    "        \n",
    "        for i in range(min(n_candidates, len(df_sorted))):\n",
    "            sample = df_sorted.iloc[i]\n",
    "            candidate = {f: sample[f] for f in features if f in df_sorted.columns}\n",
    "            candidate['strategy'] = f'top_performer_{i+1}'\n",
    "            candidate['DrumDeadT30'] = sample['DrumDeadT30']\n",
    "            candidate['DrumLiveT30'] = sample['DrumLiveT30']\n",
    "            candidates.append(candidate)\n",
    "    \n",
    "    return pd.DataFrame(candidates)\n",
    "\n",
    "# Generate candidates using different strategies\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CANDIDATE PARAMETER SETS FOR NEW EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Mean-based candidate\n",
    "candidates_mean = generate_candidate_parameters(df_success, INPUT_FEATURES, strategy='mean')\n",
    "print(\"\\n1. Based on SUCCESS CASE MEANS:\")\n",
    "display(candidates_mean.round(2))\n",
    "\n",
    "# Median-based candidate\n",
    "candidates_median = generate_candidate_parameters(df_success, INPUT_FEATURES, strategy='median')\n",
    "print(\"\\n2. Based on SUCCESS CASE MEDIANS:\")\n",
    "display(candidates_median.round(2))\n",
    "\n",
    "# Top performers\n",
    "candidates_top = generate_candidate_parameters(df_success, INPUT_FEATURES, n_candidates=5, strategy='optimized')\n",
    "print(\"\\n3. TOP PERFORMING SUCCESS CASES:\")\n",
    "display(candidates_top.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend parameter ranges based on success cases\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDED PARAMETER RANGES (from success cases)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for feature in INPUT_FEATURES:\n",
    "    if feature in df_success.columns:\n",
    "        vals = df_success[feature]\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(f\"  Range: [{vals.min():.2f}, {vals.max():.2f}]\")\n",
    "        print(f\"  Mean: {vals.mean():.2f} (±{vals.std():.2f})\")\n",
    "        print(f\"  Median: {vals.median():.2f}\")\n",
    "        print(f\"  IQR: [{vals.quantile(0.25):.2f}, {vals.quantile(0.75):.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export candidate parameters to CSV\n",
    "all_candidates = pd.concat([\n",
    "    candidates_mean,\n",
    "    candidates_median,\n",
    "    candidates_top\n",
    "], ignore_index=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = 'candidate_parameters.csv'\n",
    "all_candidates.to_csv(output_path, index=False)\n",
    "print(f\"\\nCandidate parameters saved to: {output_path}\")\n",
    "display(all_candidates.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n1. DATA OVERVIEW:\")\n",
    "print(f\"   - Total experiments: {len(df)}\")\n",
    "print(f\"   - Success cases (DrumDeadT30 < {dead_t30_threshold}ms AND DrumLiveT30 > {live_t30_threshold}ms): {len(df_success)} ({100*len(df_success)/len(df):.1f}%)\")\n",
    "\n",
    "print(f\"\\n2. KEY DIFFERENTIATING FEATURES (statistically significant):\")\n",
    "for feat in significant_features[:5]:\n",
    "    row = comparison_df[comparison_df['feature'] == feat].iloc[0]\n",
    "    direction = \"↑\" if row['diff_mean'] > 0 else \"↓\"\n",
    "    print(f\"   - {feat}: Success {direction} (diff={row['diff_mean']:.3f}, p={row['p_value']:.4f})\")\n",
    "\n",
    "print(f\"\\n3. CLASSIFIER PERFORMANCE:\")\n",
    "for name, result in results.items():\n",
    "    print(f\"   - {name}: {result['accuracy']:.1%} test accuracy, {result['cv_mean']:.1%} CV accuracy\")\n",
    "\n",
    "print(f\"\\n4. TOP FEATURES BY IMPORTANCE (Random Forest):\")\n",
    "for feat, imp in sorted(rf_importances.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "    print(f\"   - {feat}: {imp:.4f}\")\n",
    "\n",
    "print(f\"\\n5. DECISION RULES FOR SUCCESS:\")\n",
    "if success_paths:\n",
    "    top_path = sorted(success_paths, key=lambda x: x['confidence'], reverse=True)[0]\n",
    "    print(f\"   Best rule (confidence: {top_path['confidence']:.1%}):\")\n",
    "    for rule in top_path['rules']:\n",
    "        print(f\"     - {rule}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook provides a foundation for success case analysis. Future expansions could include:\n",
    "\n",
    "1. **Additional FOM combinations**: Extend analysis to other metric pairs (e.g., VocalT30, DrumDiffusion)\n",
    "2. **Multi-objective optimization**: Find Pareto-optimal configurations\n",
    "3. **Sensitivity analysis**: How robust are success cases to parameter variations?\n",
    "4. **Bayesian optimization**: Suggest optimal next experiments to run\n",
    "5. **Ensemble methods**: Combine multiple models for more robust predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
